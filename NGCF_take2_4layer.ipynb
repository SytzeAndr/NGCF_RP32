{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGCF - take2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SytzeAndr/NGCF_RP32/blob/master/NGCF_take2_4layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHUfQ4EzOos4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GuQIsFhz_O6",
        "colab_type": "code",
        "outputId": "01213845-c96c-4b98-8361-35d68520a3eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK_0Qlpt_yqp",
        "colab_type": "text"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X0FRqbX_k4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_mtx(X, n_folds=200):\n",
        "  \"\"\"\n",
        "  Split a matrix/Tensor into n parts.\n",
        "  Useful for processing large matrices in batches\n",
        "  \"\"\"\n",
        "  X_folds = []\n",
        "  fold_len = X.shape[0]//n_folds\n",
        "  for i in range(n_folds):\n",
        "    start = i * fold_len\n",
        "    if i == n_folds -1:\n",
        "      end = X.shape[0]\n",
        "    else:\n",
        "      end = (i + 1) * fold_len\n",
        "    X_folds.append(X[start:end])\n",
        "  return X_folds\n",
        "\n",
        "def to_sparse_tensor(X):\n",
        "  \"\"\"\n",
        "  Convert a sparse numpy object to a sparse pytorch tensor.\n",
        "  Note that the tensor does not yet live on the GPU\n",
        "  \"\"\"\n",
        "  coo = X.tocoo().astype(np.float32)\n",
        "  i = torch.LongTensor(np.mat((coo.row, coo.col)))\n",
        "  v = torch.FloatTensor(coo.data)\n",
        "  return torch.sparse.FloatTensor(i, v, coo.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p4cMwaf12hb",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlgYT5KcKugh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8024ce05-3c39-4ed6-905a-9b2394d5f2dc"
      },
      "source": [
        "# real data\n",
        "path = Path('./drive/My Drive/RP_data/backup')\n",
        "# toy data\n",
        "# path = Path('./drive/My Drive/RP_data/toy_data')\n",
        "train_file = path/'train.txt'\n",
        "test_file = path/'test.txt'\n",
        "\n",
        "class DataLoader:\n",
        "  def __init__(self, file, batch_size):\n",
        "    self.file = file\n",
        "    self.batch_size = batch_size\n",
        "    self.n_users, self.n_items, self.n_data = 0, 0, 0\n",
        "    self.users = []\n",
        "    self.pos_items = {}\n",
        "    self.neg_items = {}\n",
        "    self.load()\n",
        "\n",
        "  def load(self):\n",
        "    with open(self.file) as f:\n",
        "      for l in f.readlines():\n",
        "        if len(l) == 0: break\n",
        "        l = l.strip('\\n').split(' ')\n",
        "        uid = int(l[0])\n",
        "        try:\n",
        "          items = [int(i) for i in l[1:]]\n",
        "        except Exception:\n",
        "          continue\n",
        "        self.users.append(uid)\n",
        "        self.n_items = max(self.n_items, max(items))\n",
        "        self.n_users = max(self.n_users, uid)\n",
        "        self.n_data += len(items)\n",
        "        self.pos_items[uid] = items\n",
        "    self.n_users += 1\n",
        "    self.n_items += 1\n",
        "\n",
        "    # R is the Rating matrix in Dict Of Keys form, either 1. or 0. for each (user, item) pair\n",
        "    self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
        "    for u in self.users:\n",
        "      for i in self.pos_items[u]:\n",
        "        self.R[u, i] = 1.\n",
        "\n",
        "  def compute_norm_adj_matrix(self, adj):\n",
        "    # rowsum = out-degree of the node    \n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    # inverted and set to 0 if no connections\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    # sparse diagonal matrix with the normalizing factors in the diagonal\n",
        "    d_mat_inv = sp.diags(d_inv)\n",
        "    # dot product resulting in a row-normalised version of the input matrix\n",
        "    norm_adj = d_mat_inv.dot(adj)\n",
        "    return norm_adj.tocoo()\n",
        "  \n",
        "  def set_adj_matrix(self):\n",
        "    try:\n",
        "      self.adj_matrix = sp.load_npz(path/'adj_matrix.npz')\n",
        "      print('Loaded existing adj_matrix')\n",
        "    except Exception:\n",
        "      print('No exisiting adj_matrix found')\n",
        "      adj = self.compute_adj_matrix()\n",
        "      sp.save_npz(path/'adj_matrix.npz', adj.tocsr())\n",
        "      self.adj_matrix = adj\n",
        "\n",
        "  def compute_adj_matrix(self):\n",
        "    # A is the Adjecency matrix in Dict Of Keys form, used when computing the Laplacian norm\n",
        "    A = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32).tolil()\n",
        "    A[:self.n_users, self.n_users:] = self.R.tolil()\n",
        "    A[self.n_users:, :self.n_users] = self.R.tolil().T\n",
        "    A = A.todok()\n",
        "\n",
        "    # norm_adj = self.compute_norm_adj_matrix(A + sp.eye(A.shape[0]))\n",
        "    mean_adj = self.compute_norm_adj_matrix(A)\n",
        "    # L is the Laplacian used for normalizing message construction\n",
        "    # ngcf_adj = mean_adj + sp.eye(mean_adj.shape[0])\n",
        "    self.adj_matrix = mean_adj + sp.eye(mean_adj.shape[0])\n",
        "\n",
        "  def sample_pos(self, u, amount):\n",
        "    # Sample a batch of <amount> positive items for user u\n",
        "    high = len(self.pos_items[u])\n",
        "    pos_sample = []\n",
        "    while len(pos_sample) < amount:\n",
        "      id = np.random.randint(low=0, high=high, size=1)[0]\n",
        "      item = self.pos_items[u][id]\n",
        "      if item not in pos_sample:\n",
        "        pos_sample.append(item)\n",
        "    return pos_sample\n",
        "\n",
        "  def sample_neg(self, u, amount):\n",
        "    # Sample a batch of <amount> negative items for user u\n",
        "    high = self.n_items\n",
        "    neg_sample = []\n",
        "    while len(neg_sample) < amount:\n",
        "      item = np.random.randint(low=0, high=high, size=1)[0]\n",
        "      if item not in self.pos_items[u] and item not in neg_sample:\n",
        "        neg_sample.append(item)\n",
        "    return neg_sample\n",
        "\n",
        "  def sample(self):\n",
        "    # Sample a batch of batch_size users, each with a positive and negative item\n",
        "    users = np.random.choice(self.users, size=self.batch_size)\n",
        "    pos_sample, neg_sample = [], []\n",
        "    for u in users:\n",
        "      pos_sample += self.sample_pos(u, 1)\n",
        "      neg_sample += self.sample_neg(u, 1)\n",
        "    return users, pos_sample, neg_sample\n",
        "\n",
        "train_data = DataLoader(train_file, batch_size=1024)\n",
        "train_data.set_adj_matrix()\n",
        "test_data = DataLoader(test_file, batch_size=1024)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded existing adj_matrix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZpW3QFz2A7Y",
        "colab_type": "text"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ueycn8nn2DTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import init, LeakyReLU, Linear, Module, ModuleList, Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# path to save model to\n",
        "models_path = path/'models'\n",
        "\n",
        "class NGCF(Module):\n",
        "  def __init__(self, n_users, n_items, embed_size, n_layers, adj_matrix):\n",
        "    super().__init__()\n",
        "    self.n_users = n_users\n",
        "    self.n_items = n_items\n",
        "    self.embed_size = embed_size\n",
        "    self.n_layers = n_layers\n",
        "    self.adj_matrix = adj_matrix\n",
        "\n",
        "    # The (user/item)_embeddings are the initial embedding matrix E\n",
        "    self.user_embeddings = Parameter(torch.rand(n_users, embed_size))\n",
        "    self.item_embeddings = Parameter(torch.rand(n_items, embed_size))\n",
        "    # The (user/item)_embeddings_final are the final concatenated embeddings [E_1..E_L]\n",
        "    # Stored for easy tracking of final embeddings throughout optimization and eval\n",
        "    self.user_embeddings_final = Parameter(torch.zeros((n_users, embed_size * (n_layers + 1))))\n",
        "    self.item_embeddings_final = Parameter(torch.zeros((n_items, embed_size * (n_layers + 1))))\n",
        "\n",
        "    # The linear transformations for each layer\n",
        "    self.W1 = ModuleList([Linear(self.embed_size, self.embed_size) for _ in range(0, self.n_layers)])\n",
        "    self.W2 = ModuleList([Linear(self.embed_size, self.embed_size) for _ in range(0, self.n_layers)])\n",
        "\n",
        "    self.act = LeakyReLU()\n",
        "    \n",
        "    # Initialize each of the trainable weights with the Xavier initializer\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    for name, parameter in self.named_parameters():\n",
        "      if ('bias' not in name):\n",
        "        init.xavier_uniform_(parameter)\n",
        "\n",
        "  def compute_loss(self, batch_user_emb, batch_pos_emb, batch_neg_emb):\n",
        "    pos_y = torch.mul(batch_user_emb, batch_pos_emb).sum(dim=1)\n",
        "    neg_y = torch.mul(batch_user_emb, batch_neg_emb).sum(dim=1)\n",
        "    # Unregularized loss\n",
        "    bpr_loss = -(torch.log(torch.sigmoid(pos_y - neg_y))).mean()\n",
        "    return bpr_loss\n",
        "\n",
        "  def forward(self, u, i, j):\n",
        "    adj_splits = split_mtx(self.adj_matrix)\n",
        "    embeddings = torch.cat((self.user_embeddings, self.item_embeddings))\n",
        "    final_embeddings = [embeddings]\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "      embedding_parts = []\n",
        "      for part in adj_splits:\n",
        "        embedding_parts.append(torch.sparse.mm(to_sparse_tensor(part).to(device), embeddings))\n",
        "\n",
        "      # Message construction\n",
        "      t1_embeddings = torch.cat(embedding_parts, 0)\n",
        "      t1 = self.W1[l](t1_embeddings)\n",
        "      t2_embeddings = embeddings.mul(t1_embeddings)\n",
        "      t2 = self.W2[l](t2_embeddings)\n",
        "\n",
        "      # Message aggregation\n",
        "      embeddings = self.act(t1 + t2)\n",
        "      normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "      final_embeddings.append(normalized_embeddings)\n",
        "\n",
        "    # Make sure to update the (user/item)_embeddings(_final)\n",
        "    final_embeddings = torch.cat(final_embeddings, 1)\n",
        "    final_u_embeddings, final_i_embeddings = final_embeddings.split((self.n_users, self.n_items), 0)\n",
        "    self.user_embeddings_final = Parameter(final_u_embeddings)\n",
        "    self.item_embeddings_final = Parameter(final_i_embeddings)\n",
        "\n",
        "    batch_user_emb = final_u_embeddings[u]\n",
        "    batch_pos_emb = final_i_embeddings[i]\n",
        "    batch_neg_emb = final_i_embeddings[j]\n",
        "\n",
        "    return self.compute_loss(batch_user_emb, batch_pos_emb, batch_neg_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3plBLDjm5NH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "798abfb1-a545-452a-c15b-b384e5f3a218"
      },
      "source": [
        "# optional: restore previously trained model\n",
        "def restore_model(path, model, optimizer):\n",
        "  model.load_state_dict(torch.load(path/'model1.pt'))\n",
        "  optimizer.load_state_dict(torch.load(path/'optimizer.pt'))\n",
        "  print('Restored previous model')\n",
        "  return model, optimizer\n",
        "\n",
        "# norm_adj, mean_adj, ngcf_adj = train_data.compute_adj_matrix()\n",
        "device = torch.device('cuda')\n",
        "model = NGCF(n_users=train_data.n_users, n_items=train_data.n_items, embed_size=64, n_layers=2, adj_matrix=train_data.adj_matrix).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# Restore a previous model\n",
        "model, optimizer = restore_model(models_path, model, optimizer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restored previous model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHTQPAOcUD9X",
        "colab_type": "code",
        "outputId": "3c6da74b-97e0-495e-ea93-22cf5f48eae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from time import time\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "model.train()\n",
        "n_batch = train_data.n_data // train_data.batch_size + 1\n",
        "\n",
        "def compute_ndcg(top_items, test_items, test_indices, k):\n",
        "  ratings = (test_items * top_items).gather(1, test_indices)\n",
        "  norm = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
        "  dcg = (ratings / norm).sum(1)\n",
        "  dcg_max = (torch.sort(ratings, dim=1, descending=True)[0] / norm).sum(1)\n",
        "  ndcg = dcg / dcg_max\n",
        "  ndcg[torch.isnan(ndcg)] = 0\n",
        "  return ndcg\n",
        "\n",
        "def evaluate(user_embeddings, item_embeddings, k):\n",
        "  user_parts = split_mtx(user_embeddings)\n",
        "  train_parts = split_mtx(train_data.R)\n",
        "  test_parts = split_mtx(test_data.R)\n",
        "\n",
        "  recall_parts, ndcg_parts = [], []\n",
        "\n",
        "  for user_part, train_part, test_part in zip(user_parts, train_parts, test_parts):\n",
        "\n",
        "    # Get the prediction scores for the users and items as a cuda float\n",
        "    non_train_items = torch.from_numpy(1 - (train_part.todense())).float().to(device)\n",
        "    predictions = torch.mm(user_part, item_embeddings.t()) * non_train_items\n",
        "    # Get the k highest scores, scatter them as a float tensor in the GPU\n",
        "    _, test_indices = torch.topk(predictions, dim=1, k=k)\n",
        "    top_items = torch.zeros_like(predictions).float()\n",
        "    top_items.scatter_(dim=1, index=test_indices, src=torch.tensor(1.0).to(device))\n",
        "  \n",
        "    test_items = torch.from_numpy(test_part.todense()).float().to(device)\n",
        "    TP = (test_items * top_items).sum(1)\n",
        "    recall = TP / test_items.sum(1)\n",
        "    recall[torch.isnan(recall)] = 1\n",
        "    ndcg = compute_ndcg(top_items, test_items, test_indices, k)\n",
        "  \n",
        "    recall_parts.append(recall)\n",
        "    ndcg_parts.append(ndcg)\n",
        "\n",
        "  mean_recall = torch.cat(recall_parts).mean()\n",
        "  mean_ndcg = torch.cat(ndcg_parts).mean()\n",
        "  print('Recall:\\t' + str(mean_recall.item()))\n",
        "  print('NDCG\\t:' + str(mean_ndcg.item()))\n",
        "\n",
        "def save_state(model, optimizer, epoch):\n",
        "  torch.save(model.state_dict(), models_path/'model1.pt')\n",
        "  torch.save(optimizer.state_dict(), models_path/'optimizer.pt')\n",
        "  torch.save(torch.IntTensor(epoch), models_path/'epoch.pt')\n",
        "\n",
        "def train(model, data, t):\n",
        "  total_loss = 0\n",
        "  start = time()\n",
        "  timings = []\n",
        "  for b in range(100):\n",
        "    batch_start = time()\n",
        "    u, i, j = data.sample()\n",
        "    u = torch.from_numpy(u).long().to(device)\n",
        "    i = torch.LongTensor(i).to(device)\n",
        "    j = torch.LongTensor(j).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(u, i, j)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    timings.append(time()-batch_start)\n",
        "\n",
        "  avg_batch = np.average(timings)\n",
        "  print('Finished epoch ' + str(t+1) + ' in\\t' + str(time()-start) + ' sec')\n",
        "  print('Total BPR loss:\\t\\t' + str(total_loss))\n",
        "  print('Average batch time:\\t' + str(avg_batch))\n",
        "\n",
        "for t in range(n_epochs):\n",
        "  print('Starting epoch: ' + str(t+1))\n",
        "  train(model, train_data, t)\n",
        "  save_state(model, optimizer, t)\n",
        "  if (t+1) % 5 == 0:\n",
        "    model.eval()\n",
        "    evaluate(model.user_embeddings_final.detach(), model.item_embeddings_final.detach(), k=20)\n",
        "    model.train()\n",
        "  print('\\n============\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch: 1\n",
            "Finished epoch 1 in\t748.3205902576447 sec\n",
            "Total BPR loss:\t\t20.049512788653374\n",
            "Average batch time:\t7.48320125579834\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 2\n",
            "Finished epoch 2 in\t747.4265201091766 sec\n",
            "Total BPR loss:\t\t19.296502575278282\n",
            "Average batch time:\t7.4742617464065555\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 3\n",
            "Finished epoch 3 in\t748.0292520523071 sec\n",
            "Total BPR loss:\t\t18.557230845093727\n",
            "Average batch time:\t7.480289001464843\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 4\n",
            "Finished epoch 4 in\t748.0652596950531 sec\n",
            "Total BPR loss:\t\t17.659307405352592\n",
            "Average batch time:\t7.480648710727691\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 5\n",
            "Finished epoch 5 in\t748.0515072345734 sec\n",
            "Total BPR loss:\t\t17.122695237398148\n",
            "Average batch time:\t7.480511629581452\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([263])\n",
            "torch.Size([306])\n",
            "Recall:\tnan\n",
            "NDCG\t:0.04501167684793472\n",
            "\n",
            "============\n",
            "\n",
            "Starting epoch: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9_dY7MpZy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2764ac84-26b0-49b5-d2ad-60d34a538b3d"
      },
      "source": [
        "def compute_ndcg(top_items, test_items, test_indices, k):\n",
        "  ratings = (test_items * top_items).gather(1, test_indices)\n",
        "  norm = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
        "  dcg = (ratings / norm).sum(1)\n",
        "  dcg_max = (torch.sort(ratings, dim=1, descending=True)[0] / norm).sum(1)\n",
        "  ndcg = dcg / dcg_max\n",
        "  ndcg[torch.isnan(ndcg)] = 0\n",
        "  return ndcg\n",
        "\n",
        "def evaluate(user_embeddings, item_embeddings, k):\n",
        "  user_parts = split_mtx(user_embeddings)\n",
        "  train_parts = split_mtx(train_data.R)\n",
        "  test_parts = split_mtx(test_data.R)\n",
        "\n",
        "  recall_parts, ndcg_parts = [], []\n",
        "\n",
        "  for user_part, train_part, test_part in zip(user_parts, train_parts, test_parts):\n",
        "\n",
        "    # Get the prediction scores for the users and items as a cuda float\n",
        "    non_train_items = torch.from_numpy(1 - (train_part.todense())).float().to(device)\n",
        "    predictions = torch.mm(user_part, item_embeddings.t()) * non_train_items\n",
        "    # Get the k highest scores, scatter them as a float tensor in the GPU\n",
        "    _, test_indices = torch.topk(predictions, dim=1, k=k)\n",
        "    top_items = torch.zeros_like(predictions).float()\n",
        "    top_items.scatter_(dim=1, index=test_indices, src=torch.tensor(1.0).to(device))\n",
        "  \n",
        "    test_items = torch.from_numpy(test_part.todense()).float().to(device)\n",
        "    TP = (test_items * top_items).sum(1)\n",
        "    recall = TP / test_items.sum(1)\n",
        "    recall[torch.isnan(recall)] = 1\n",
        "    ndcg = compute_ndcg(top_items, test_items, test_indices, k)\n",
        "  \n",
        "    recall_parts.append(recall)\n",
        "    ndcg_parts.append(ndcg)\n",
        "\n",
        "  mean_recall = torch.cat(recall_parts).mean()\n",
        "  mean_ndcg = torch.cat(ndcg_parts).mean()\n",
        "  print('Recall:\\t' + str(mean_recall.item()))\n",
        "  print('NDCG:\\t' + str(mean_ndcg.item()))\n",
        "\n",
        "evaluate(model.user_embeddings_final.detach(), model.item_embeddings_final.detach(), k=20)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall:\t0.015093409456312656\n",
            "NDCG\t:0.04501167684793472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUl49uXH17W2",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2P6ncEkiE2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_mtx(X, n_folds=100):\n",
        "  # Split a matrix/Tensor into n_folds (for the user embeddings and the R matrices)\n",
        "  X_folds = []\n",
        "  fold_len = X.shape[0]//n_folds\n",
        "  for i in range(n_folds):\n",
        "    start = i * fold_len\n",
        "    if i == n_folds -1:\n",
        "      end = X.shape[0]\n",
        "    else:\n",
        "      end = (i + 1) * fold_len\n",
        "    X_folds.append(X[start:end])\n",
        "  return X_folds\n",
        "\n",
        "def ndcg_at_k_gpu(pred_items, test_items, test_indices, k):\n",
        "  # to calculate ndcg@k\n",
        "  r = (test_items * pred_items).gather(1, test_indices)\n",
        "  f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().cuda()\n",
        "  dcg = (r[:, :k]/f).sum(1)\n",
        "  dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)\n",
        "  ndcg = dcg/dcg_max\n",
        "  ndcg[torch.isnan(ndcg)] = 0\n",
        "  return ndcg\n",
        "\n",
        "def test_GPU(u_emb, i_emb, Rtr, Rte, Ks):\n",
        "  ue_folds = split_mtx(u_emb)\n",
        "  tr_folds = split_mtx(Rtr)\n",
        "  te_folds = split_mtx(Rte)\n",
        "\n",
        "  fold_prec, fold_rec, fold_ndcg, fold_hr = \\\n",
        "    defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
        "  for ue_f, tr_f, te_f in zip(ue_folds, tr_folds, te_folds):\n",
        "    scores = torch.mm(ue_f, i_emb.t())\n",
        "    test_items = torch.from_numpy(te_f.todense()).float().cuda()\n",
        "    non_train_items = torch.from_numpy(1-(tr_f.todense())).float().cuda()\n",
        "    scores = scores * non_train_items\n",
        "    _, test_indices = torch.topk(scores, dim=1, k=max(Ks))\n",
        "    pred_items = torch.zeros_like(scores).float()\n",
        "    pred_items.scatter_(dim=1,index=test_indices,src=torch.tensor(1.0).cuda())\n",
        "\n",
        "    for k in Ks:\n",
        "      topk_preds = torch.zeros_like(scores).float()\n",
        "      topk_preds.scatter_(dim=1,index=test_indices[:, :k],src=torch.tensor(1.0))\n",
        "\n",
        "      TP = (test_items * topk_preds).sum(1)\n",
        "      prec = TP/k\n",
        "      rec = TP/test_items.sum(1)\n",
        "      hit_r = (TP > 0).float()\n",
        "      ndcg = ndcg_at_k_gpu(pred_items, test_items, test_indices, k)\n",
        "\n",
        "      fold_prec[k].append(prec)\n",
        "      fold_rec[k].append(rec)\n",
        "      fold_ndcg[k].append(ndcg)\n",
        "      fold_hr[k].append(hit_r)\n",
        "\n",
        "  result = {'precision': [], 'recall': [], 'ndcg': [], 'hit_ratio': []}\n",
        "  for k in Ks:\n",
        "    result['precision'].append(torch.cat(fold_prec[k]).mean())\n",
        "    result['recall'].append(torch.cat(fold_rec[k]).mean())\n",
        "    result['ndcg'].append(torch.cat(fold_ndcg[k]).mean())\n",
        "    result['hit_ratio'].append(torch.cat(fold_hr[k]).mean())\n",
        "  return result\n",
        "\n",
        "def early_stopping(log_value, best_value, stopping_step, expected_order='asc', patience=10):\n",
        "  # better is higher or lower\n",
        "  assert expected_order in ['asc', 'dec']\n",
        "  if (expected_order == 'asc' and log_value >= best_value) or (expected_order == 'dec' and log_value <= best_value):\n",
        "    stopping_step = 0\n",
        "    best_value = log_value\n",
        "  else:\n",
        "    stopping_step += 1\n",
        "  if stopping_step >= patience:\n",
        "    print(\"Early stopping is trigger at step: {} log:{}\".format(patience, log_value))\n",
        "    should_stop = True\n",
        "  else:\n",
        "    should_stop = False\n",
        "  return best_value, stopping_step, should_stop\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5O4b6tWqeQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with early stopping\n",
        "print_every, eval_every, save_every = 1, 1, 10\n",
        "Ks = [10, 20]\n",
        "\n",
        "model = NGCF(n_users=1000, n_items=2000, embed_size=64, n_layers=2, adj_matrix=train_data.L)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
        "n_epochs = 20\n",
        "\n",
        "model.train()\n",
        "n_batch = train_data.n_data // train_data.batch_size + 1\n",
        "print('Total batches: ' + str(n_batch))\n",
        "\n",
        "cur_best_loss, stopping_step, should_stop = 1e3, 0, False\n",
        "\n",
        "for t in range(n_epochs):\n",
        "  epoch_loss = train(model, train_data)\n",
        "  print(str(t) + ': ' + str(epoch_loss))\n",
        "  if epoch % eval_every  == (eval_every - 1):\n",
        "    result = test_GPU(model.user_embeddings_final.detach(), model.item_embeddings_final.detach(), train_data.R, test_data.R, Ks)\n",
        "    log_value = result['recall'][0]\n",
        "    cur_best_metric, stopping_step, should_stop = early_stopping(log_value, cur_best_metric, stopping_step, args.patience)\n",
        "  if should_stop == True: \n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}