{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGCF - take2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPJCfPWM5U4J1hx49/pqM/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SytzeAndr/NGCF_RP32/blob/master/NGCF_take2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHUfQ4EzOos4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GuQIsFhz_O6",
        "colab_type": "code",
        "outputId": "cdc3e75b-6c53-47dc-c62b-23aab081f09b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p4cMwaf12hb",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlgYT5KcKugh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = Path('./drive/My Drive/RP_data/backup')\n",
        "train_file = path/'train.txt'\n",
        "test_file = path/'test.txt'\n",
        "\n",
        "class DataLoader:\n",
        "  def __init__(self, file, batch_size):\n",
        "    self.file = file\n",
        "    self.batch_size = batch_size\n",
        "    self.n_users, self.n_items, self.n_data = 0, 0, 0\n",
        "    self.users = []\n",
        "    self.pos_items = {}\n",
        "    self.neg_items = {}\n",
        "    self.load()\n",
        "    self.compute_adj_matrix()\n",
        "\n",
        "  def load(self):\n",
        "    with open(self.file) as f:\n",
        "      for l in f.readlines():\n",
        "        if len(l) == 0: break\n",
        "        l = l.strip('\\n').split(' ')\n",
        "        uid = int(l[0])\n",
        "        items = [int(i) for i in l[1:]]\n",
        "        self.users.append(uid)\n",
        "        self.n_items = max(self.n_items, max(items))\n",
        "        self.n_users = max(self.n_users, uid)\n",
        "        self.n_data += len(items)\n",
        "        self.pos_items[uid] = items\n",
        "    self.n_users += 1\n",
        "    self.n_items += 1\n",
        "\n",
        "    # R is the Rating matrix in Dict Of Keys form, either 1. or 0. for each (user, item) pair\n",
        "    self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
        "    for u in self.users:\n",
        "      for i in self.pos_items[u]:\n",
        "        self.R[u, i] = 1.\n",
        "\n",
        "  def compute_norm_adj_matrix(self, adj):\n",
        "    # rowsum = out-degree of the node    \n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    # inverted and set to 0 if no connections\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    # sparse diagonal matrix with the normalizing factors in the diagonal\n",
        "    d_mat_inv = sp.diags(d_inv)\n",
        "    # dot product resulting in a row-normalised version of the input matrix\n",
        "    norm_adj = d_mat_inv.dot(adj)\n",
        "    return norm_adj.tocoo()\n",
        "  \n",
        "  def compute_adj_matrix(self):\n",
        "    # A is the Adjecency matrix in Dict Of Keys form, used when computing the Laplacian norm\n",
        "    A = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
        "    A[:self.n_users, self.n_users:] = self.R.tolil()\n",
        "    A[self.n_users:, :self.n_users] = self.R.tolil().T\n",
        "    A = A.todok()\n",
        "\n",
        "    norm_adj = self.compute_norm_adj_matrix(A + sp.eye(A.shape[0]))\n",
        "    mean_adj = self.compute_norm_adj_matrix(A)\n",
        "    # L is the Laplacian used for normalizing message construction\n",
        "    self.L = norm_adj + sp.eye(mean_adj.shape[0])\n",
        "\n",
        "  def sample_pos(self, u, amount):\n",
        "    # Sample a batch of <amount> positive items for user u\n",
        "    high = len(self.pos_items[u])\n",
        "    pos_sample = []\n",
        "    while len(pos_sample) < amount:\n",
        "      id = np.random.randint(low=0, high=high, size=1)[0]\n",
        "      item = self.pos_items[u][id]\n",
        "      if item not in pos_sample:\n",
        "        pos_sample.append(item)\n",
        "    return pos_sample\n",
        "\n",
        "  def sample_neg(self, u, amount):\n",
        "    # Sample a batch of <amount> negative items for user u\n",
        "    high = self.n_items\n",
        "    neg_sample = []\n",
        "    while len(neg_sample) < amount:\n",
        "      item = np.random.randint(low=0, high=high, size=1)[0]\n",
        "      if item not in self.pos_items[u] and item not in neg_sample:\n",
        "        neg_sample.append(item)\n",
        "    return neg_sample\n",
        "\n",
        "  def sample(self):\n",
        "    # Sample a batch of batch_size users, each with a positive and negative item\n",
        "    users = np.random.choice(self.users, size=self.batch_size)\n",
        "    pos_sample, neg_sample = [], []\n",
        "    for u in users:\n",
        "      pos_sample += self.sample_pos(u, 1)\n",
        "      neg_sample += self.sample_neg(u, 1)\n",
        "    return users, pos_sample, neg_sample\n",
        "\n",
        "train_data = DataLoader(train_file, 1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjbVncq33l8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.compute_adj_matrix()\n",
        "users, pos, neg = train_data.sample()\n",
        "print(len(users), len(pos), len(neg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZpW3QFz2A7Y",
        "colab_type": "text"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ueycn8nn2DTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.nn import init, LeakyReLU, Linear, Module, ModuleList, Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NGCF(nn.Module):\n",
        "  def __init__(self, n_users, n_items, embed_size, n_layers, adj_matrix):\n",
        "    super().__init__()\n",
        "    self.n_users = n_users\n",
        "    self.n_items = n_items\n",
        "    self.embed_size = embed_size\n",
        "    self.n_layers = n_layers\n",
        "    self.adj_matrix = adj_matrix\n",
        "\n",
        "    # The (user/item)_embeddings are the initial embedding matrix E\n",
        "    self.user_embeddings = Parameter(torch.rand(n_users, embed_size))\n",
        "    self.item_embeddings = Parameter(torch.rand(n_items, embed_size))\n",
        "    # The (user/item)_embeddings_final are the final concatenated embeddings [E_1..E_L]\n",
        "    # Stored for easy tracking of final embeddings throughout optimization and eval\n",
        "    self.user_embeddings_final = Parameter(torch.zeros((n_users, embed_size * (n_layers + 1))))\n",
        "    self.item_embeddings_final = Parameter(torch.zeros((n_items, embed_size * (n_layers + 1))))\n",
        "\n",
        "    # The linear transformations for each layer\n",
        "    self.W1 = ModuleList([Linear(self.embed_size, self.embed_size) for _ in range(0, self.n_layers)])\n",
        "    self.W2 = ModuleList([Linear(self.embed_size, self.embed_size) for _ in range(0, self.n_layers)])\n",
        "\n",
        "    self.act = LeakyReLU()\n",
        "    \n",
        "    # Initialize each of the trainable weights with the Xavier initializer\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    for name, parameter in self.named_parameters():\n",
        "      if ('bias' not in name):\n",
        "        init.xavier_uniform_(parameter)\n",
        "\n",
        "  def to_sparse_tensor(self, X):\n",
        "    coo = X.tocoo().astype(np.float32)\n",
        "    i = torch.LongTensor(np.mat((coo.row, coo.col)))\n",
        "    v = torch.FloatTensor(coo.data)\n",
        "    return torch.sparse.FloatTensor(i, v, coo.shape)\n",
        "\n",
        "  def compute_loss(self, u, i, j):\n",
        "    # Get the embeddings final embeddings for the current sample\n",
        "    batch_user_emb = self.user_embeddings_final[u]\n",
        "    batch_pos_emb = self.item_embeddings_final[i]\n",
        "    batch_neg_emb = self.item_embeddings_final[j]\n",
        "\n",
        "    pos_y = torch.mul(batch_user_emb, batch_pos_emb).sum(dim=1)\n",
        "    neg_y = torch.mul(batch_user_emb, batch_neg_emb).sum(dim=1)\n",
        "    # Unregularized loss\n",
        "    bpr_loss = -(torch.log(torch.sigmoid(pos_y - neg_y))).mean()\n",
        "    return bpr_loss\n",
        "\n",
        "  def forward(self, u, i, j):\n",
        "    embeddings = torch.cat((self.user_embeddings, self.item_embeddings))\n",
        "    final_embeddings = [embeddings]\n",
        "\n",
        "    for l in range(self.n_layers):\n",
        "\n",
        "      # Message construction\n",
        "      t1_embeddings = torch.sparse.mm(self.to_sparse_tensor(self.adj_matrix), embeddings)\n",
        "      t1 = self.W1[l](t1_embeddings)\n",
        "      t2_embeddings = embeddings.mul(t1_embeddings)\n",
        "      t2 = self.W2[l](t2_embeddings)\n",
        "\n",
        "      # Message aggregation\n",
        "      embeddings = self.act(t1 + t2)\n",
        "      normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "      final_embeddings.append(normalized_embeddings)\n",
        "\n",
        "    # Make sure to update the (user/item)_embeddings(_final)\n",
        "    final_embeddings = torch.cat(final_embeddings, 1)\n",
        "    final_u_embeddings, final_i_embeddings = final_embeddings.split((self.n_users, self.n_items), 0)\n",
        "    self.user_embeddings_final = Parameter(final_u_embeddings)\n",
        "    self.item_embeddings_final = Parameter(final_i_embeddings)\n",
        "\n",
        "    return self.compute_loss(u, i, j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHTQPAOcUD9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "4657a598-6282-48d2-8997-876e1ef7bd58"
      },
      "source": [
        "model = NGCF(n_users=1000, n_items=2000, embed_size=64, n_layers=2, adj_matrix=train_data.L)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
        "n_epochs = 20\n",
        "\n",
        "def train(model, data):\n",
        "  total_loss = 0\n",
        "  for b in range(n_batch):\n",
        "    u, i, j = data.sample()\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(u, i, j)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  return total_loss\n",
        "\n",
        "model.train()\n",
        "n_batch = train_data.n_data // train_data.batch_size + 1\n",
        "print('Total batches: ' + str(n_batch))\n",
        "for t in range(n_epochs):\n",
        "  epoch_loss = train(model, train_data)\n",
        "  print(str(t) + ': ' + str(epoch_loss))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total batches: 24\n",
            "0: 16.40497100353241\n",
            "1: 16.412854433059692\n",
            "2: 16.407753586769104\n",
            "3: 16.401634633541107\n",
            "4: 16.40473961830139\n",
            "5: 16.40420436859131\n",
            "6: 16.406798183918\n",
            "7: 16.398602187633514\n",
            "8: 16.410856127738953\n",
            "9: 16.409800827503204\n",
            "10: 16.40645980834961\n",
            "11: 16.405922412872314\n",
            "12: 16.408011853694916\n",
            "13: 16.401700973510742\n",
            "14: 16.40281391143799\n",
            "15: 16.410841941833496\n",
            "16: 16.405292451381683\n",
            "17: 16.406582713127136\n",
            "18: 16.404249131679535\n",
            "19: 16.408159732818604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUl49uXH17W2",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1EaXjVnyMOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall_at_k(pred, k, interactions):\n",
        "  pred = np.asfarray(pred)[:k]\n",
        "  return np.sum(pred) / interactions\n",
        "\n",
        "def dcg_at_k(pred, k):\n",
        "  pred = np.asfarray(pred)[:k]\n",
        "  return np.sum(pred / np.log2(np.arange(2, pred.size + 2)))\n",
        "\n",
        "def ndcg_at_k(pred, k):\n",
        "  max_dcg = dcg_at_k(sorted(pred, reverse=True), k)\n",
        "  return dcg_at_k(pred, k) / max_dcg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rkykAgxzXqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b67e9e88-b52c-42d9-9af2-68b6dbfced67"
      },
      "source": [
        "r = np.random.choice(2, 20, p=[0.7, 0.3])\n",
        "k = 10\n",
        "interactions = 20\n",
        "print(recall_at_k(r, k, interactions))\n",
        "print(ndcg_at_k(r, k))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2\n",
            "0.3995166168199184\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}