{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGCF.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SytzeAndr/NGCF_RP32/blob/master/NGCF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frx1uCxdNYWU",
        "colab_type": "text"
      },
      "source": [
        "##The steps to take\n",
        "As a rough outline, we can sketch the steps to be taken to train a Neural Graph Collaborative Filtering system as follows.\n",
        "\n",
        "1. Parse the data in $(u, i, j)$ tuples, where we have user $u$, positive item $i$ and negative item $j$.\n",
        "1. Implement the NGCF model\n",
        "  1. Create a user-item interaction graph for each batch.\n",
        "  1. Create a NGCF Layer that can be used to perform a MessagePassing step.\n",
        "  1. Use this NGCF Layer in a Neural Net to obtain predictions for the batch.\n",
        "1. Train all the parameters\n",
        "  1. Construct the matrices W1_l, W2_l and E_l for each step l, initialize them with the Xavier Initializer.\n",
        "  1. Sample a batch from the training data. Apply L steps of message passing, and save all the matrices W1_{1..L}, W2_{1..L}, E_{1..L}, repeat until all training data are sampled once.\n",
        "  1. Construct the final embeddings for each user and item, by concatenating the embeddings for a user or item from each step, so **e_u** = **e_u^1** || ... || **e_u^L**\n",
        "  1. Using the final embeddings, compute the prediction for each (user, item) pair, and compute the BPR loss.\n",
        "  1. Use the BPR loss to update the matrices W1 and W2, repeat until $T$ epochs are done.\n",
        "1. Generate the test output, compute recall and ndgc, compare them to the Table 3\n",
        "  1. Use the trained matrices W1 and W2 to find the prediction values for each (user, item) pair in the test data.\n",
        "  1. Compute the recall and normalized discounted cumulative gain (ndgc)\n",
        "\n",
        "Since we have to compare results for step l, and the final embeddings are calculated by concatenation over the steps, we can simply perform all L steps (4 in this case), and then do the analysis over subsets of the W1 and W2 matrices.\n",
        "\n",
        "Hyperparameters to consider:\n",
        " - Negative slope of NGCFLayer#LeakyReLU\n",
        " - Learning rate for the Adam optimizer\n",
        " - $\\lambda$ in loss computation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni4rEp2I06Rd",
        "colab_type": "text"
      },
      "source": [
        "##File loading\n",
        "\n",
        "Here we use the Google Drive mountpoint to load files. For this to work, note the following:\n",
        "\n",
        "\n",
        "\n",
        "*   The first time you execute this, it will provide a link, which you need to follow and give permission for Colab to access your Google Drive.\n",
        "*   Make sure that the data is located in the folder `RP_data` which should be located in the root of your Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GuQIsFhz_O6",
        "colab_type": "code",
        "outputId": "9c7d741b-5a43-45a6-dbd9-ab1386328269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "data_path = './drive/My Drive/RP_data/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z0bqsE0VsKc",
        "colab_type": "text"
      },
      "source": [
        "## Loading pytorch with torch_geometric (PyG)\n",
        "Some of its steps are described in this blog post:\n",
        "https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8\n",
        "\n",
        "There is also a google colab which trains a GCN to identify 'spammers'\n",
        "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Deep_GCN_Spam.ipynb#scrollTo=_4_eVOI2M4Uo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sUDqw3zY7AD",
        "colab_type": "text"
      },
      "source": [
        "**Importing torch_geometric 1.3.2**\n",
        "\n",
        "`torch_geometric` is a geometric deep learning extension library for PyTorch. We don't use the latest version (loaded by default by google colab) due to inconsistencies with PyTorch. This also means we have to downgrade a few other packages aswell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC_b_4uOGthh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "146eb07f-e46b-4047-cc70-55d1720a917e"
      },
      "source": [
        "pip install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch===1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 23kB/s \n",
            "\u001b[?25hCollecting torchvision===0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/e6/a564eba563f7ff53aa7318ff6aaa5bd8385cbda39ed55ba471e95af27d19/torchvision-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (8.8MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch===1.2.0) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision===0.4.0) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision===0.4.0) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "  Found existing installation: torchvision 0.5.0\n",
            "    Uninstalling torchvision-0.5.0:\n",
            "      Successfully uninstalled torchvision-0.5.0\n",
            "Successfully installed torch-1.2.0 torchvision-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mo9WkaqnqEI",
        "colab_type": "code",
        "outputId": "e3e4dafa-4076-41e7-adf2-b7ad2d2cb2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# these were the corresponding versions for torch-geometric 1.3.2, released 4 oct 2019, which runs on torch 1.2.0.\n",
        "# grab some coffee, this might take a while\n",
        "!pip install torch-scatter==1.3.1\n",
        "!pip install torch-sparse==0.4.0\n",
        "!pip install torch-cluster==1.4.4\n",
        "!pip install torch-spline-conv==1.1.0\n",
        "!pip install torch-geometric==1.3.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-scatter==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d4/750403a8aa32cdb3d2d05849c6a10e4e0604de5e0cc94b81a0d0d69a75f3/torch_scatter-1.3.1.tar.gz\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-1.3.1-cp36-cp36m-linux_x86_64.whl size=2724863 sha256=016eafd42f98edb05c0a2bfa2330047595a5265c8f572b9222a7a4e33e980ff1\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/21/0b/c42fa9353ceec5e87464599e470a03e4250ec667b4a392fa7d\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-1.3.1\n",
            "Collecting torch-sparse==0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/0a/2ff678e0d04e524dd2cf990a6202ced8c0ffe3fe6b08e02f25cc9fd27da0/torch_sparse-0.4.0.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==0.4.0) (1.18.2)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.4.0-cp36-cp36m-linux_x86_64.whl size=3552041 sha256=e450dfbeb3ff5d42e589a19fa09d05c044a8fda72e62192206764eccbe5ddf97\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/83/0a/38ea460df5586a075b877fe089619e5238487712a0645940bd\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.4.0\n",
            "Collecting torch-cluster==1.4.4\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/5f/01c5799cd1f81f9956f03a0e1d9a861e020a598dd411d9bd3c3c1dd5b8a4/torch_cluster-1.4.4.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-cluster==1.4.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-cluster==1.4.4) (1.18.2)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.4.4-cp36-cp36m-linux_x86_64.whl size=14436836 sha256=4213c2e381ec451eb5957735cbb7d48a5f651019c23cd37f821769cf7aed5408\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/7b/ab/b3e266920055d1e51988f93a99ef8df62e399b234c8d50527f\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.4.4\n",
            "Collecting torch-spline-conv==1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/dd/daa9d0b7b2ede913e573876ae286a58ec296678858f2814ff6d6789b234f/torch_spline_conv-1.1.0.tar.gz\n",
            "Building wheels for collected packages: torch-spline-conv\n",
            "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.1.0-cp36-cp36m-linux_x86_64.whl size=4971705 sha256=09c7cd6836a46a8b01d0adf13d572e9ae6b5161f7ccee50b99a8cb054b89ab56\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/00/1f/9c414a9a5f340dd8d2e5e362d1bb5cad91fc7aec78c935fd66\n",
            "Successfully built torch-spline-conv\n",
            "Installing collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.1.0\n",
            "Collecting torch-geometric==1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/50/0a802f0bfa68058bf025d219ec6fbe806a5b891bba6702e28be7b83679fb/torch_geometric-1.3.2.tar.gz (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric==1.3.2) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric==1.3.2) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric==1.3.2) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric==1.3.2) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric==1.3.2) (2.21.0)\n",
            "Collecting plyfile\n",
            "  Downloading https://files.pythonhosted.org/packages/93/c8/cf47848cd4d661850e4a8e7f0fc4f7298515e06d0da7255ed08e5312d4aa/plyfile-0.7.2-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric==1.3.2) (1.0.3)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 31.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric==1.3.2) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric==1.3.2) (0.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric==1.3.2) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric==1.3.2) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric==1.3.2) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric==1.3.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric==1.3.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric==1.3.2) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric==1.3.2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric==1.3.2) (2018.9)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric==1.3.2) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch-geometric==1.3.2) (1.12.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.3.2-cp36-none-any.whl size=203339 sha256=c4f305efd7a28ea28cc5ce3a224ff43ec82aae31342cbecb2189a7a1a2fa175e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/75/0a/56a0fd58efac6d990782523e20e61c9307fc42c31564d40348\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: plyfile, isodate, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 plyfile-0.7.2 rdflib-4.2.2 torch-geometric-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErcV2T7PWLDd",
        "colab_type": "code",
        "outputId": "edf6febb-b5f2-4cd8-c6d2-23c341bda463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "import torch_cluster\n",
        "import torch_geometric\n",
        "\n",
        "# we use torch version 1.2.0 instead of the latest due to dependency errors\n",
        "print(torch.__version__)\n",
        "# verify that torch geometric is imported, should be 1.3.2\n",
        "print(torch_geometric.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2.0\n",
            "1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn_xzVLBUaMS",
        "colab_type": "text"
      },
      "source": [
        "# Data loading\n",
        "The model will be trained on batches of $(u,i,j)$, where $u$ is a user, $i$ is an item with which $u$ interacted and $j$ is an item with which $u$ did not interact. Therefore, when loading the data, we find all the possible $(u,i,j)$ tuples and store them. The `DataConstruction` class makes data handling and parsing easier, since the processed data is stored in a separate file for easy access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50A2DprzbPKc",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, how about we load the data as Graph, slice it up (can be done as in the BiPartite section) and then serve that to the Net. Then, when computing the loss, find the triplets in the batch and compute the BPR loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdbZmKcZbx2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "153ca685-0c3f-48b9-8dc1-018cc59332ba"
      },
      "source": [
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class Dataset(InMemoryDataset):\n",
        "  def __init__(self, root, transform=None, pre_transform=None):\n",
        "    super(Dataset, self).__init__(root, transform, pre_transform)\n",
        "    self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    return []\n",
        "  \n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    return ['processed.dataset']\n",
        "  \n",
        "  def download(self):\n",
        "    pass\n",
        "  \n",
        "  def process(self):\n",
        "    users = []\n",
        "    items = []\n",
        "    pos_items = {}\n",
        "    with open(os.path.join(self.root, 'data.txt')) as f:\n",
        "      for l in f.readlines():\n",
        "        if len(l) > 0:\n",
        "          l = l.strip('\\n')\n",
        "          items = [int(i) for i in l.split(' ')]\n",
        "          uid, items = items[0], items[1:]\n",
        "          users.append(uid)\n",
        "          items.extend(items)\n",
        "          pos_items[uid] = np.asarray(items)\n",
        "    \n",
        "    users = torch.tensor(users, dtype=torch.long)\n",
        "    items = torch.unique(torch.tensor(items, dtype=torch.long))\n",
        "    n_users = users.size(0)\n",
        "    n_items = items.size(0)\n",
        "\n",
        "    edges = []\n",
        "    for user in users:\n",
        "      uid = user.item()\n",
        "      for item in pos_items[uid]:\n",
        "        edges.append([uid, item + n_users])\n",
        "        edges.append([item + n_users, uid])\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    \n",
        "    x = torch.cat((users, items + n_users))\n",
        "    datalist = [Data(x=x, edge_index=edge_index)]\n",
        "    data, slices = self.collate(datalist)\n",
        "    torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "train_path = data_path + 'train2'\n",
        "train_data = Dataset(root=train_path)\n",
        "print(train_data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset(1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed1gVEhhqbZt",
        "colab_type": "text"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDlCsjl7Spjl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "For each user-item pair (u,i), we define the message from i to u as\n",
        "\n",
        "$m_{u \\leftarrow i} = \\dfrac{1}{\\sqrt{|N_u||N_i|}} (W_1 e_i + W_2(e_i \\odot e_u))$\n",
        "\n",
        "Where $N_u, N_i$ are the first hop neighbors of $u$ and $i$, $W_1, W_2$ are trainable weight matrices to distill useful information for propagation, and $e_i$ and $e_u$ are embeddings of the users and items. Note that in the special case where $u == i$, the second term is dropped.\n",
        "\n",
        "The embedding for a user (or item) is updated through message aggregation as \n",
        "\n",
        "$e_u^{(l)} = LeakyReLU(m_{u \\leftarrow u} + \\sum_{i \\in N_u} m_{u \\leftarrow i})$\n",
        "\n",
        "If the graph is constructed properly, this is simply a summation over all edges (when selfloops are included), followed by a LeakyReLU activiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHwG78ZV3-4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import init, Embedding, LeakyReLU, Linear, ModuleList\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "\n",
        "# define our NGCF layer\n",
        "class NGCFLayer(MessagePassing):\n",
        "  def __init__(self, embed_size):\n",
        "    super(NGCFLayer, self).__init__(aggr='add') # Summation as aggregation\n",
        "    # The W1 matrix\n",
        "    self.lin1 = Linear(embed_size, embed_size, bias=False)\n",
        "    init.xavier_normal_(self.lin1.weight)\n",
        "    # The W2 matrix\n",
        "    self.lin2 = Linear(embed_size, embed_size, bias=False)\n",
        "    init.xavier_normal_(self.lin2.weight)\n",
        "    self.act = LeakyReLU()\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    # Add self loops\n",
        "    print(edge_index.shape)\n",
        "    edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "    print(x.shape)\n",
        "\n",
        "    # Compute normalization\n",
        "    row, col = edge_index\n",
        "    print(row.shape)\n",
        "    deg = degree(row, x.size(0), dtype=x.dtype)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "    return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x, norm=norm)\n",
        "  \n",
        "  def message(self, x_i, x_j, norm):\n",
        "    # Construct message\n",
        "    message = self.lin1(x_j)\n",
        "    # Only add the second term if it is not a self loop\n",
        "    if x_i.data_ptr() != x_j.data_ptr():\n",
        "      message += self.lin2(torch.mul(x_i, x_j))\n",
        "    return message\n",
        "\n",
        "  def update(self, aggr_out):\n",
        "    # Return the LeakyReLU result\n",
        "    return self.act(aggr_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P49KKqy7Np7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 64\n",
        "batch_size = 1024\n",
        "num_epochs = 3\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n_layers, num_embeddings):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.embedding = Embedding(num_embeddings=num_embeddings, embedding_dim=embed_size)\n",
        "    init.xavier_uniform_(self.embedding.weight)\n",
        "\n",
        "    self.ngcf_layers = ModuleList([NGCFLayer(embed_size=embed_size) for i in range(n_layers)])\n",
        "    \n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    x = self.embedding(x)\n",
        "    for layer in self.ngcf_layers:\n",
        "      x = layer(x, edge_index)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54GuypLWrCu0",
        "colab_type": "text"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcqVxTDk8FDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to enable running on the GPU in google colab: \n",
        "# Runtime -> select Change runtime type -> select GPU and hit Save\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Net(n_layers=1, num_embeddings=torch.max(train_data.data.x) + 1)\n",
        "data = train_data.data\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxk0C8UVWd4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "52fa0622-356a-4ff2-9d29-5fe16db38fc0"
      },
      "source": [
        "# Utility to parse a batch of triplets into a Graph structure\n",
        "def parse_batch(batch):\n",
        "  # define an edge for each (u, i)\n",
        "  edge_index = torch.empty((2, batch_size*2), dtype=torch.long)\n",
        "  for i in range(batch_size):\n",
        "    edge_index[0, 2*i] = edge_index[1, 2*i+1] = batch[i, 0]\n",
        "    edge_index[1, 2*i] = edge_index[0, 2*i+1] = batch[i, 1] + train_data.n_users\n",
        "  \n",
        "  # list all user and item identifiers as data\n",
        "  users = batch[:, 0]\n",
        "  items = torch.unique(batch[:, 1:].flatten()) + train_data.n_users\n",
        "  x = torch.cat((users, items))\n",
        "  \n",
        "  return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad()\n",
        "output = model(data)\n",
        "print(output)\n",
        "\n",
        "\n",
        "#     loss = crit(output, label)\n",
        "#     loss.backward()\n",
        "#     loss_all += dataset.num_graphs * loss_item()\n",
        "#     optimizer.step()\n",
        "#   return loss_all / len(train_loader)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 9522920])\n",
            "torch.Size([52667, 64])\n",
            "torch.Size([9575587])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-dfd65d8a115b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-875d6bdd9c85>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngcf_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3c3c6ea20cea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/utils/degree.py\u001b[0m in \u001b[0;36mdegree\u001b[0;34m(index, num_nodes, dtype)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mnum_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_num_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Invalid index in scatterAdd at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:536"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFlr-rxYnsid",
        "colab_type": "text"
      },
      "source": [
        "#Legacy\n",
        "\n",
        "The DataReader class is a utility class that will help loading the data, compute useful properties of the data and create sample batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "20UOLdQ9rdyX",
        "colab": {}
      },
      "source": [
        "import random as rd\n",
        "\n",
        "class DataReader(object):\n",
        "  def __init__(self, path='./drive/My Drive/RP_data/raw', batch_size=1024):\n",
        "    self.path = path\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    train_file = path + '/train.txt'\n",
        "    test_file = path + '/test.txt'\n",
        "\n",
        "    self.n_train = 0\n",
        "    self.n_test = 0\n",
        "    self.exist_users = []\n",
        "    self.train_items = {}\n",
        "    self.test_items = {}\n",
        "\n",
        "    with open(train_file) as f:\n",
        "      for l in f.readlines():\n",
        "        if len(l) > 0:\n",
        "          l = l.strip('\\n')\n",
        "          items = [int(i) for i in l.split(' ')]\n",
        "          uid, train_items = items[0], items[1:]\n",
        "          self.exist_users.append(uid)\n",
        "          self.train_items[uid] = train_items\n",
        "          self.n_train += len(train_items)\n",
        "\n",
        "    with open(test_file) as f:\n",
        "      for l in f.readlines():\n",
        "        if len(l) > 0:\n",
        "          l = l.strip('\\n')\n",
        "          try:\n",
        "            items = [int(i) for i in l.split(' ')]\n",
        "            uid, test_items = items[0], items[1:]\n",
        "            self.exist_users.append(uid)\n",
        "            self.test_items[uid] = test_items\n",
        "            self.n_test += len(test_items)\n",
        "          except Exception:\n",
        "            continue\n",
        "\n",
        "    train_max_item = max([max(items) for items in list(self.train_items.values())])\n",
        "    test_max_item = max([max(items) for items in list(self.test_items.values())])\n",
        "    self.n_items = max(train_max_item, test_max_item)\n",
        "    self.n_users = max(self.exist_users)\n",
        "  \n",
        "\n",
        "  def sample(self):\n",
        "    if self.batch_size <= self.n_users:\n",
        "      users = rd.sample(self.exist_users, self.batch_size)\n",
        "    else:\n",
        "      users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
        "\n",
        "\n",
        "    def sample_pos_items_for_u(u, num):\n",
        "      pos_items = self.train_items[u]\n",
        "      n_pos_items = len(pos_items)\n",
        "      pos_batch = []\n",
        "      while True:\n",
        "        if len(pos_batch) == num: break\n",
        "        pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
        "        pos_i_id = pos_items[pos_id]\n",
        "\n",
        "        if pos_i_id not in pos_batch:\n",
        "          pos_batch.append(pos_i_id)\n",
        "      return pos_batch\n",
        "\n",
        "\n",
        "    def sample_neg_items_for_u(u, num):\n",
        "      neg_batch = []\n",
        "      while True:\n",
        "        if len(neg_batch) == num: break\n",
        "        neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
        "        if neg_id not in self.train_items[u] and neg_id not in neg_batch:\n",
        "          neg_batch.append(neg_id)\n",
        "      return neg_batch\n",
        "    \n",
        "\n",
        "    pos_items, neg_items = [], []\n",
        "    for u in users:\n",
        "      pos_items += sample_pos_items_for_u(u, 1)\n",
        "      neg_items += sample_neg_items_for_u(u, 1)\n",
        "\n",
        "    return users, pos_items, neg_items\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2iFaHvdvc8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# small test to verify whether we can load data\n",
        "# dataReader = DataReader()\n",
        "# dataReader.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBcxk1KyA9zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch_geometric.data.Data provides a number of utility functions\n",
        "# these prints are a check to verify if our graph seems valid\n",
        "# print(data.is_undirected())\n",
        "# print(data.is_directed())\n",
        "# print(data.num_edges)\n",
        "# print(data.num_nodes)\n",
        "# print(data.contains_isolated_nodes())\n",
        "# print(data.contains_self_loops())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4xtsOWbHSkO",
        "colab_type": "text"
      },
      "source": [
        "First we initialize the initial weights and embeddings. This is performed by [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3YA6bVLWv-b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def xavier(m, h):\n",
        "  return torch.Tensor(m, h).uniform_(-1, 1)*math.sqrt(6./(m+h))\n",
        "\n",
        "class NGCF(object):\n",
        "  def __init__(self, n_users, n_items, emb_dim=64, n_layers=4):\n",
        "    self.n_users = n_users\n",
        "    self.n_items = n_items\n",
        "    self.emb_dim = emb_dim\n",
        "    self.weight_size = list(np.repeat(emb_dim, n_layers))\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "  def _init_weights(self):\n",
        "    all_weights = dict()\n",
        "    all_weights['user_embedding'] = xavier(self.n_users, self.emb_dim) \n",
        "    all_weights['item_embedding'] = xavier(self.n_items, self.emb_dim)\n",
        "    self.weight_size_list = [self.emb_dim] + self.weight_size\n",
        "    for k in range(self.n_layers):\n",
        "      all_weights['W_gc_%d' % k] = xavier(self.weight_size_list[k], self.weight_size_list[k+1])\n",
        "      all_weights['b_gc_%d' % k] = xavier(1, self.weight_size_list[k+1])\n",
        "      all_weights['W_bi_%d' % k] = xavier(self.weight_size_list[k], self.weight_size_list[k + 1])\n",
        "      all_weights['b_bi_%d' % k] = xavier(1, self.weight_size_list[k + 1])\n",
        "      all_weights['W_mlp_%d' % k] = xavier(self.weight_size_list[k], self.weight_size_list[k+1])\n",
        "      all_weights['b_mlp_%d' % k] = xavier(1, self.weight_size_list[k+1])\n",
        "    return all_weights\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxRyv2Lh3pg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set initial weight\n",
        "# emd_dim and n_layers can be tweaked to our liking\n",
        "# ncfg_obj = NGCF(n_users=dataReader.n_users, n_items=dataReader.n_items, emb_dim=64, n_layers=4)\n",
        "\n",
        "# store all our initial weights in some dict object\n",
        "# ncfg_weights = ncfg_obj._init_weights()\n",
        "\n",
        "# optional prints: check the amount of user embeddings as a verification\n",
        "# print(ncfg_weights.keys())\n",
        "# print(len(ncfg_weights['user_embedding']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "31Ka5fcKV_dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class DataConstruction(InMemoryDataset):\n",
        "  def __init__(self, root, transform=None, pre_transform=None):\n",
        "    super(DataConstruction, self).__init__(root, transform, pre_transform)\n",
        "    self.data = torch.load(self.processed_paths[0])\n",
        "    self.n_users = torch.max(self.data[:, 0]).item()\n",
        "    self.n_items = torch.max(self.data[:, 1:]).item()\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    return [os.path.join(self.root, 'data.txt')]\n",
        "  \n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    return ['processed.dataset']\n",
        "  \n",
        "  def download(self):\n",
        "    pass\n",
        "  \n",
        "  def process(self):\n",
        "    data = []\n",
        "\n",
        "    for file in self.raw_file_names:\n",
        "      users = []\n",
        "      items = []\n",
        "      pos_items = {}\n",
        "\n",
        "      with open(file) as f:\n",
        "        for l in f.readlines():\n",
        "          if len(l) > 0:\n",
        "            l = l.strip('\\n')\n",
        "            items = [int(i) for i in l.split(' ')]\n",
        "            uid, items = items[0], items[1:]\n",
        "            users.append(uid)\n",
        "            items.extend(items)\n",
        "            pos_items[uid] = np.asarray(items)\n",
        "      users = np.asarray(users)\n",
        "      items = np.unique(items)\n",
        "\n",
        "      for uid in users:\n",
        "        pos = pos_items[uid]\n",
        "        neg = np.setdiff1d(items, pos)\n",
        "        for p in pos:\n",
        "          for n in neg:\n",
        "            data.append([uid, p, n])\n",
        "\n",
        "    torch.save(torch.tensor(data), self.processed_paths[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}